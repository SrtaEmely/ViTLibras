"""
###############################################################
#Import for neural network
###############################################################
"""
from keras import backend as K
from keras.models import load_model
from keras.models import model_from_json
import json
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder
# Import necessary components for face detection
import numpy as np
import pandas as pd
import cv2
import dlib
#import tensorflow.compat.v1 as tf
#tf.disable_v2_behavior() 
# Import necessary components for creation of xml file
from xml.etree import ElementTree, cElementTree
from xml.dom import minidom
import xml.etree.ElementTree as etree
import xml.etree.ElementTree as et
from xml.etree.ElementTree import tostring
from skimage.util import random_noise
"""
##############################################################
#Entry
##############################################################
"""
'''
x_u_disfa = np.load("D:/Experimento_final/Data_annotations/x_u_disfa.npy")
x_u_hm = np.load('D:/Experimento_final/Data_annotations/x_u_HM.npy')
x_u_silfa = np.load('D:/Experimento_final/Data_annotations/x_u_silfa.npy')
#
y_u_disfa = np.load("D:/Experimento_final/Data_annotations/y_u_disfa.npy")
y_u_hm = np.load('D:/Experimento_final/Data_annotations/y_u_HM.npy')
y_u_silfa = np.load('D:/Experimento_final/Data_annotations/y_u_silfa.npy')
##############################################################################
##############################################################################
x_=random_noise(x_u_disfa, mode='gaussian', var=0.05**2)
x_u_disfa=np.append(x_u_disfa,x_)
y_u_disfa=np.append(y_u_disfa,y_u_disfa)

X_u=np.append(np.append(x_u_disfa,x_u_hm),x_u_silfa)
Y_u=np.append(np.append(y_u_disfa,y_u_hm[:int(x_u_hm.size/(60*97*1))]),y_u_silfa[:int(x_u_silfa.size/(60*97*1))])

#X_u=np.append(x_u_hm,x_u_silfa)
#Y_u=np.append(y_u_hm[:int(x_u_hm.size/(60*97*1))],y_u_silfa[:int(x_u_silfa.size/(60*97*1))])

Y_u=np.nan_to_num(Y_u)
#Y_u=Y_u.astype('float32')
X_u=np.nan_to_num(X_u)
X_u = X_u.astype('float32')
##############################################################################
##############################################################################
img_rows_u, img_cols_u = 60, 97
X_u = X_u.reshape(int(X_u.shape[0]/(60*97*1)), img_rows_u, img_cols_u, 1)
input_shape = (img_rows_u, img_cols_u, 1)
##############################################################################
##############################################################################
# convert class vectors to binary class matrices
encoder_u = LabelEncoder()
encoder_u.fit(Y_u)
encoded_Y_u = encoder_u.transform(Y_u)
# convert integers to dummy variables (i.e. one hot encoded)
Y_u = np_utils.to_categorical(encoded_Y_u)
num_classes_u = Y_u.shape[1]
print(num_classes_u)
labels_encoded_u=encoder_u.inverse_transform(encoded_Y_u)
labels_ordered_u=np.sort(labels_encoded_u)
labels_ordered_u=np.append(labels_ordered_u,74)
labels_ordered_u=set(labels_ordered_u)
labels_ordered_u=np.fromiter(labels_ordered_u, int, len(labels_ordered_u))
'''
##############################################################################
##############################################################################

#Load and prepare the dataset
x_l = np.load('D:/Experimento_final/Data_annotations/x_l_disfa.npy')
x_l_train1 = np.load('D:/Experimento_final/Data_annotations/x_l_HM.npy')
x_l_train2 = np.load('D:/Experimento_final/Data_annotations/x_l_silfa.npy')
#
y_l = np.load('D:/Experimento_final/Data_annotations/y_l_disfa.npy')
y_l_train1 = np.load('D:/Experimento_final/Data_annotations/y_l_HM.npy')
y_l_train2 = np.load('D:/Experimento_final/Data_annotations/y_l_silfa.npy')

##############################################################################
##############################################################################
x_=random_noise(x_l, mode='gaussian', var=0.05**2)
x_l=np.append(x_l,x_)
X_l=np.append(np.append(x_l,x_l_train1),x_l_train2)
#X_l=np.append(x_l_train1,x_l_train2)
X_l=np.nan_to_num(X_l)
X_l[np.isneginf(X_l)]=0.0
X_l[np.isposinf(X_l)]=0.0
X_l = X_l.astype('float32')
#
y_l=np.append(y_l,y_l)
Y_l=np.append(np.append(y_l,y_l_train1[:int(x_l_train1.size/(36*98*1))]),y_l_train2[:int(x_l_train2.size/(36*98*1))])
#Y_l=np.append(y_l_train1[:int(x_l_train1.size/(36*98*1))],y_l_train2[:int(x_l_train2.size/(36*98*1))])
Y_l=np.nan_to_num(Y_l)
Y_l[np.isneginf(Y_l)]=0
Y_l[np.isposinf(Y_l)]=0
#Y_l=Y_l.astype('float32')

img_rows, img_cols = 36, 98
X_l = X_l.reshape(int(X_l.shape[0]/(36*98*1)), img_rows, img_cols, 1)
input_shape_l = (img_rows, img_cols, 1)

##############################################################################
##############################################################################
# convert class vectors to binary class matrices
encoder = LabelEncoder()
encoder.fit(Y_l)
encoded_Y_train = encoder.transform(Y_l)
# convert integers to dummy variables (i.e. one hot encoded)
Y = np_utils.to_categorical(encoded_Y_train)
#
num_classes_l = Y.shape[1]
#
#x_train, x_test, y_train, y_test = sk.train_test_split(X_u, Y, train_size = 0.9, random_state = seed(2018))
#x_, x_test, y_, y_test=sk.train_test_split(X_l,Y, test_size=0.3, random_state=13)

x_min = X_l.min( keepdims=True)
x_max = X_l.max( keepdims=True)
X_l = (X_l - x_min)/(x_max-x_min)

##############################################################################
##############################################################################
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import sklearn.model_selection as sk


num_classes = num_classes_l
input_shape = (36, 98, 1)

learning_rate = 0.001
weight_decay = 0.0001
batch_size = 12
num_epochs = 1
image_size = 72  # We'll resize input images to this size
patch_size = 6  # Size of the patches to be extract from the input images
num_patches = (image_size // patch_size) ** 2
projection_dim = 64
num_heads = 4
transformer_units = [
    projection_dim * 2,
    projection_dim,
]  # Size of the transformer layers
transformer_layers = 8
mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier

x_train, x_test, y_train, y_test=sk.train_test_split(X_l,Y, test_size=0.2, random_state=11)

data_augmentation = keras.Sequential(
    [
        layers.Normalization(),
        layers.Resizing(image_size, image_size),
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(factor=0.02),
        layers.RandomContrast(factor=0.2),
        layers.RandomBrightness([-0.8,0.8]),
        layers.RandomTranslation(height_factor=0.2, width_factor=0.2),
        layers.RandomZoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name="data_augmentation",
)
# Compute the mean and the variance of the training data for normalization.
data_augmentation.layers[0].adapt(x_train)

def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=tf.nn.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)
    return x

def get_f1(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches
    
    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'patch_size': self.patch_size,
            })
        return config
    
class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded
    
    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'num_patches': self.num_patches,
            })
        return config

def create_vit_classifier():
    inputs = layers.Input(shape=input_shape)
    # Augment data.
    augmented = data_augmentation(inputs)
    # Create patches.
    patches = Patches(patch_size)(augmented)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    # Classify outputs.
    logits = layers.Dense(num_classes)(features)
    # Create the Keras model.
    model = tf.keras.Model(inputs=inputs, outputs=logits)
    return model

model = create_vit_classifier()

optimizer = tfa.optimizers.AdamW(
    learning_rate=learning_rate, weight_decay=weight_decay
)

model.compile(
    optimizer=optimizer,
    loss=keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=[
        keras.metrics.CategoricalAccuracy(name="accuracy"),
        keras.metrics.MeanAbsoluteError(name='mean_absolute_error'),
        keras.metrics.TopKCategoricalAccuracy(k=5, name='top_k_categorical_accuracy'),
        get_f1,
        #keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
    ],
)

checkpoint_filepath = "C:/Users/estre/OneDrive/Documentos/docs/vit_saved"
checkpoint_callback = keras.callbacks.ModelCheckpoint(
    checkpoint_filepath,
    monitor="val_accuracy",
    save_best_only=True,
    save_weights_only=True,
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=batch_size,
    epochs=num_epochs,
    validation_split=0.2,
    callbacks=[checkpoint_callback],
)

#model.load_weights(checkpoint_filepath)
#_, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)
#print(f"Test accuracy: {round(accuracy * 100, 2)}%")
#print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")


model.save_weights('C:/Users/estre/OneDrive/Documentos/docs/vit_saved/vit_l.h5py')
model.save('C:/Users/estre/OneDrive/Documentos/docs/vit_saved/vit_l.h5')

model_json = model.to_json()
with open("C:/Users/estre/OneDrive/Documentos/docs/vit_saved/vit_l.json", "w") as json_file:
    json_file.write(model_json)
